{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 of ys3031, EECS 6894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "model = keras.applications.resnet50.ResNet50(include_top=True,\n",
    "                                            weights='imagenet',\n",
    "                                            input_tensor=None,\n",
    "                                            input_shape=None,\n",
    "                                            pooling=None,\n",
    "                                            classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1=9472+256\n",
    "conv2a=4160+256+36928+256+16640+16640+1024+1024\n",
    "conv2b=16448+256+36928+256+16640+1024 \n",
    "conv2c= 16448+256+36928+256+16640+1024\n",
    "conv3a= 32896+512+147584+512+66048+131584+2048+2048\n",
    "conv3b= 65664+512+147584+512+66048+2048\n",
    "conv3c= 65664+512+147584+512+66048+2048                                                                 \n",
    "conv3d= 65664+512+147584+512+66048+2048\n",
    "conv4a= 131328+1024+590080+1024+263168+525312+4096+4096\n",
    "conv4b= 262400+1024+590080+1024+263168+4096\n",
    "conv4c= 262400+1024+590080+1024+263168+4096                                                                 \n",
    "conv4d= 262400+1024+590080+1024+263168+4096\n",
    "conv4e= 262400+1024+590080+1024+263168+4096\n",
    "conv4f= 262400+1024+590080+1024+263168+4096\n",
    "conv5a= 524800+2048+2359808+2048+1050624+2099200+8192+8192                                                                 \n",
    "conv5b= 1049088+2048+2359808+2048+1050624+8192                                                                 \n",
    "conv5c= 1049088+2048+2359808+2048+1050624+8192                                                                 \n",
    "convfc= 2049000\n",
    "\n",
    "conv2 = conv2a+conv2b+conv2c\n",
    "conv3 = conv3a+conv3b+conv3c+conv3c\n",
    "conv4 = conv4a+conv4b+conv4c+conv4d+conv4e+conv4f\n",
    "conv5 = conv5a+conv5b+conv5c\n",
    "\n",
    "print(\"conv1:\", conv1)\n",
    "print(\"conv2:\", conv2)\n",
    "print(\"conv3:\", conv3)\n",
    "print(\"conv4:\", conv4)\n",
    "print(\"conv5:\", conv5)\n",
    "\n",
    "print(conv1+conv2a+conv2b+conv2c+conv3a+conv3b+conv3c+conv3d+conv4a+conv4b+conv4c+conv4d+conv4e+conv4f+conv5a+conv5b+conv5c+convfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_origin/train/jennifer/1-FaceId-0.jpg\n",
      "data_origin/train/jennifer/6-FaceId-0.jpg\n",
      "data_origin/train/jennifer/7-FaceId-0.jpg\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import os\n",
    "\n",
    "def data_aug(category, starname):\n",
    "    for filename in os.listdir(os.path.join(\"data_origin\", category, starname)):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            filenameandpath = os.path.join(\"data_origin\", category, starname, filename)\n",
    "            print(filenameandpath)\n",
    "            img = load_img(os.path.join(\"data_origin\", category, starname, filename))\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = img_array.reshape((1,) + img_array.shape)\n",
    "            i = 0\n",
    "            datagen = ImageDataGenerator(\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "            for _ in datagen.flow(img_array, \n",
    "                                  batch_size=1, \n",
    "                                  save_to_dir=os.path.join(\"data_expanded\", category, starname), \n",
    "                                  save_prefix=starname, \n",
    "                                  save_format=\"jpeg\"):\n",
    "                i += 1\n",
    "                if i > 20:\n",
    "                    break\n",
    "            # for pic in os.listdir(os.path.join(\"data_expanded\", category, starname)):\n",
    "            #     if pic.endswith(\".jpeg\"):\n",
    "            #         expanded_img_array = vgg_pre.predict(img_to_array(load_img(os.path.join(\"data_exapnded\", category, \n",
    "            #                                                                 starname, pic), target_size=(224,224,3))))\n",
    "                \n",
    "                \n",
    "                    \n",
    "data_aug(\"train\", \"jennifer\")\n",
    "data_aug(\"validation\", \"jennifer\")\n",
    "data_aug(\"train\", \"justin\")\n",
    "data_aug(\"validation\", \"justin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 397 images belonging to 2 classes.\n",
      "Found 377 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 638s 16s/step - loss: 2.2811 - acc: 0.7248 - val_loss: 0.1877 - val_acc: 0.9178\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = 'data_expanded/train'\n",
    "validation_data_dir = 'data_expanded/validation'\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "model_vgg = VGG16(input_shape=input_shape, include_top=False)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(model_vgg)\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
    "top_model.add(Dense(256))\n",
    "top_model.add(Activation('relu'))\n",
    "top_model.add(Dropout(0.8))\n",
    "top_model.add(Dense(1))\n",
    "top_model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(top_model)\n",
    "\n",
    "model.layers[0].trainable = False \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "\n",
    "top_model.save_weights(\"./top_model_pre_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 397 images belonging to 2 classes.\n",
      "Found 377 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=10, validation_data=<keras.pre..., steps_per_epoch=50, validation_steps=40)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 425s 8s/step - loss: 0.1523 - acc: 0.9398 - val_loss: 0.2111 - val_acc: 0.9055\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 421s 8s/step - loss: 0.1389 - acc: 0.9468 - val_loss: 0.2009 - val_acc: 0.9055\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 429s 9s/step - loss: 0.1153 - acc: 0.9618 - val_loss: 0.2011 - val_acc: 0.9144\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 419s 8s/step - loss: 0.1149 - acc: 0.9600 - val_loss: 0.1946 - val_acc: 0.9181\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 439s 9s/step - loss: 0.1145 - acc: 0.9588 - val_loss: 0.2207 - val_acc: 0.8992\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 421s 8s/step - loss: 0.1117 - acc: 0.9646 - val_loss: 0.2168 - val_acc: 0.9005\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 422s 8s/step - loss: 0.1023 - acc: 0.9658 - val_loss: 0.2236 - val_acc: 0.9018\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 423s 8s/step - loss: 0.0798 - acc: 0.9807 - val_loss: 0.2159 - val_acc: 0.9093\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 425s 9s/step - loss: 0.1136 - acc: 0.9670 - val_loss: 0.2177 - val_acc: 0.9030\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 419s 8s/step - loss: 0.1051 - acc: 0.9626 - val_loss: 0.2059 - val_acc: 0.9118\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "\n",
    "top_model_weights_path = 'top_model_pre_weights.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = 'data_expanded/train'\n",
    "validation_data_dir = 'data_expanded/validation'\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "nb_train_samples = 2000 // batch_size\n",
    "nb_validation_samples = 800 // batch_size\n",
    "\n",
    "\n",
    "# build the VGG16 network\n",
    "vgg_model = VGG16(include_top=False, input_shape=(img_width,img_height,3))\n",
    "print('Model loaded.')\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
    "top_model.add(Dense(256))\n",
    "top_model.add(Activation('relu'))\n",
    "top_model.add(Dropout(0.8))\n",
    "top_model.add(Dense(1))\n",
    "top_model.add(Activation('sigmoid'))\n",
    "\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# # add the model on top of the convolutional base\n",
    "model = Sequential()\n",
    "model.add(vgg_model)\n",
    "model.add(top_model)\n",
    "\n",
    "# # set the first 25 layers (up to the last conv block)\n",
    "# # to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# # compile the model with a SGD/momentum optimizer\n",
    "# # and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# # prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# # fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=1000,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)\n",
    "\n",
    "model.save_weights(\"fine_tuned_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_selfie = load_img('./selfie/sunyiming.jpeg', target_size=(150, 150))\n",
    "selfie_array = img_to_array(img_selfie)\n",
    "selfie_array = selfie_array.reshape((1,) + selfie_array.shape)\n",
    "selfie_array /= 255.0\n",
    "result = model.predict_classes(selfie_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "je_pic = load_img('./selfie/jennifer.jpeg', target_size=(224, 224))\n",
    "je_array = img_to_array(je_pic)\n",
    "print(je_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# instantiate model\n",
    "keras.backend.set_learning_phase(0)\n",
    "kmodel = ResNet50(weights='imagenet')\n",
    "preprocessing = (np.array([104, 116, 123]), 1)\n",
    "fmodel = foolbox.models.KerasModel(kmodel, bounds=(0, 255), preprocessing=preprocessing)\n",
    "\n",
    "# get source image and label\n",
    "image, label = foolbox.utils.imagenet_example()\n",
    "print(image.shape, type(image))\n",
    "print(je_array.shape, type(image))\n",
    "\n",
    "# apply attack on source image\n",
    "# ::-1 reverses the color channels, because Keras ResNet50 expects BGR instead of RGB\n",
    "attack = foolbox.attacks.FGSM(fmodel)\n",
    "adversarial = attack(je_array[:, :, ::-1], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
